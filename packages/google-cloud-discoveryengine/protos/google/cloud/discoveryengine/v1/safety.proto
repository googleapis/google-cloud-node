// Copyright 2025 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

syntax = "proto3";

package google.cloud.discoveryengine.v1;

import "google/api/field_behavior.proto";

option csharp_namespace = "Google.Cloud.DiscoveryEngine.V1";
option go_package = "cloud.google.com/go/discoveryengine/apiv1/discoveryenginepb;discoveryenginepb";
option java_multiple_files = true;
option java_outer_classname = "SafetyProto";
option java_package = "com.google.cloud.discoveryengine.v1";
option objc_class_prefix = "DISCOVERYENGINE";
option php_namespace = "Google\\Cloud\\DiscoveryEngine\\V1";
option ruby_package = "Google::Cloud::DiscoveryEngine::V1";

// Harm categories that will block the content.
enum HarmCategory {
  // The harm category is unspecified.
  HARM_CATEGORY_UNSPECIFIED = 0;

  // The harm category is hate speech.
  HARM_CATEGORY_HATE_SPEECH = 1;

  // The harm category is dangerous content.
  HARM_CATEGORY_DANGEROUS_CONTENT = 2;

  // The harm category is harassment.
  HARM_CATEGORY_HARASSMENT = 3;

  // The harm category is sexually explicit content.
  HARM_CATEGORY_SEXUALLY_EXPLICIT = 4;

  // The harm category is civic integrity.
  HARM_CATEGORY_CIVIC_INTEGRITY = 5;
}

// Safety rating corresponding to the generated content.
message SafetyRating {
  // Harm probability levels in the content.
  enum HarmProbability {
    // Harm probability unspecified.
    HARM_PROBABILITY_UNSPECIFIED = 0;

    // Negligible level of harm.
    NEGLIGIBLE = 1;

    // Low level of harm.
    LOW = 2;

    // Medium level of harm.
    MEDIUM = 3;

    // High level of harm.
    HIGH = 4;
  }

  // Harm severity levels.
  enum HarmSeverity {
    // Harm severity unspecified.
    HARM_SEVERITY_UNSPECIFIED = 0;

    // Negligible level of harm severity.
    HARM_SEVERITY_NEGLIGIBLE = 1;

    // Low level of harm severity.
    HARM_SEVERITY_LOW = 2;

    // Medium level of harm severity.
    HARM_SEVERITY_MEDIUM = 3;

    // High level of harm severity.
    HARM_SEVERITY_HIGH = 4;
  }

  // Output only. Harm category.
  HarmCategory category = 1 [(google.api.field_behavior) = OUTPUT_ONLY];

  // Output only. Harm probability levels in the content.
  HarmProbability probability = 2 [(google.api.field_behavior) = OUTPUT_ONLY];

  // Output only. Harm probability score.
  float probability_score = 5 [(google.api.field_behavior) = OUTPUT_ONLY];

  // Output only. Harm severity levels in the content.
  HarmSeverity severity = 6 [(google.api.field_behavior) = OUTPUT_ONLY];

  // Output only. Harm severity score.
  float severity_score = 7 [(google.api.field_behavior) = OUTPUT_ONLY];

  // Output only. Indicates whether the content was filtered out because of this
  // rating.
  bool blocked = 3 [(google.api.field_behavior) = OUTPUT_ONLY];
}
