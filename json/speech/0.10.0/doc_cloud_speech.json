{"id":"speech/v1/doc/doc_cloud_speech","type":"class","overview":"<p>\n  This class allows you interact with Cloud Speech API.\n</p>\n\n\n\n<p>\n  First, install <code>@google-cloud/speech</code> with npm:\n</p>\n\n<div hljs language=\"bash\">$ npm install --save @google-cloud/speech</div>\n\n<p>\n  If you are running your app on Google Compute Engine, you won't need to worry about supplying connection configuration options to <code>@google-cloud/speech</code>â€” we figure that out for you.\n</p>\n\n<p>\n  However, if you're running your app elsewhere, you will need to provide project details to authenticate API requests.\n</p>\n\n<h4>Google Cloud Platform</h4>\n<div hljs language=\"javascript\">\nvar speech = require('@google-cloud/speech')();\n</div>\n\n<h4>Elsewhere</h4>\n<div hljs language=\"javascript\">\nvar speech = require('@google-cloud/speech')({\n  projectId: 'grape-spaceship-123',\n  keyFilename: '/path/to/keyfile.json'\n});\n</div>\n\n<p>\n  The full set of options which can be passed to <code>@google-cloud/speech</code> are outlined in our <a href=\"#/docs/speech/v0.10.0/guides/authentication\">Authentication guide</a>.\n</p>\n","source":"packages\\speech\\src\\v1\\doc\\doc_cloud_speech.js","parent":"speech","children":[],"methods":[{"id":"RecognizeRequest","name":"RecognizeRequest","type":"instance","description":"","source":"packages\\speech\\src\\v1\\doc\\doc_cloud_speech.js#L39","resources":[],"examples":[],"params":[{"name":"config","description":"<p> <em>Required</em> Provides information to the recognizer that specifies how to  process the request.</p><p> This object should have the same structure as <a ui-sref=\"docs.service({\n          serviceId: '{{ service.parent }}/{{service.path.split('/').shift()}}/data_types',\n          method: 'RecognitionConfig'\n        })\">RecognitionConfig</a></p>","types":["Object"],"optional":false,"nullable":false},{"name":"audio","description":"<p> <em>Required</em> The audio data to be recognized.</p><p> This object should have the same structure as <a ui-sref=\"docs.service({\n          serviceId: '{{ service.parent }}/{{service.path.split('/').shift()}}/data_types',\n          method: 'RecognitionAudio'\n        })\">RecognitionAudio</a></p>","types":["Object"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"LongRunningRecognizeRequest","name":"LongRunningRecognizeRequest","type":"instance","description":"","source":"packages\\speech\\src\\v1\\doc\\doc_cloud_speech.js#L61","resources":[],"examples":[],"params":[{"name":"config","description":"<p> <em>Required</em> Provides information to the recognizer that specifies how to  process the request.</p><p> This object should have the same structure as <a ui-sref=\"docs.service({\n          serviceId: '{{ service.parent }}/{{service.path.split('/').shift()}}/data_types',\n          method: 'RecognitionConfig'\n        })\">RecognitionConfig</a></p>","types":["Object"],"optional":false,"nullable":false},{"name":"audio","description":"<p> <em>Required</em> The audio data to be recognized.</p><p> This object should have the same structure as <a ui-sref=\"docs.service({\n          serviceId: '{{ service.parent }}/{{service.path.split('/').shift()}}/data_types',\n          method: 'RecognitionAudio'\n        })\">RecognitionAudio</a></p>","types":["Object"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"StreamingRecognizeRequest","name":"StreamingRecognizeRequest","type":"instance","description":"","source":"packages\\speech\\src\\v1\\doc\\doc_cloud_speech.js#L92","resources":[],"examples":[],"params":[{"name":"streamingConfig","description":"<p> Provides information to the recognizer that specifies how to process the  request. The first <code>StreamingRecognizeRequest</code> message must contain a  <code>streaming_config</code> message.</p><p> This object should have the same structure as <a ui-sref=\"docs.service({\n          serviceId: '{{ service.parent }}/{{service.path.split('/').shift()}}/data_types',\n          method: 'StreamingRecognitionConfig'\n        })\">StreamingRecognitionConfig</a></p>","types":["Object"],"optional":false,"nullable":false},{"name":"audioContent","description":"<p> The audio data to be recognized. Sequential chunks of audio data are sent  in sequential <code>StreamingRecognizeRequest</code> messages. The first  <code>StreamingRecognizeRequest</code> message must not contain <code>audio_content</code> data  and all subsequent <code>StreamingRecognizeRequest</code> messages must contain  <code>audio_content</code> data. The audio bytes must be encoded as specified in  <code>RecognitionConfig</code>. Note: as with all bytes fields, protobuffers use a  pure binary representation (not base64). See  <a href=\"https://cloud.google.com/speech/limits#content\">audio limits</a>.</p>","types":["string"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"StreamingRecognitionConfig","name":"StreamingRecognitionConfig","type":"instance","description":"<p>Provides information to the recognizer that specifies how to process the request.</p>","source":"packages\\speech\\src\\v1\\doc\\doc_cloud_speech.js#L128","resources":[],"examples":[],"params":[{"name":"config","description":"<p> <em>Required</em> Provides information to the recognizer that specifies how to  process the request.</p><p> This object should have the same structure as <a ui-sref=\"docs.service({\n          serviceId: '{{ service.parent }}/{{service.path.split('/').shift()}}/data_types',\n          method: 'RecognitionConfig'\n        })\">RecognitionConfig</a></p>","types":["Object"],"optional":false,"nullable":false},{"name":"singleUtterance","description":"<p> <em>Optional</em> If <code>false</code> or omitted, the recognizer will perform continuous  recognition (continuing to wait for and process audio even if the user  pauses speaking) until the client closes the input stream (gRPC API) or  until the maximum time limit has been reached. May return multiple  <code>StreamingRecognitionResult</code>s with the <code>is_final</code> flag set to <code>true</code>.</p><p> If <code>true</code>, the recognizer will detect a single spoken utterance. When it  detects that the user has paused or stopped speaking, it will return an  <code>END_OF_SINGLE_UTTERANCE</code> event and cease recognition. It will return no  more than one <code>StreamingRecognitionResult</code> with the <code>is_final</code> flag set to  <code>true</code>.</p>","types":["boolean"],"optional":false,"nullable":false},{"name":"interimResults","description":"<p> <em>Optional</em> If <code>true</code>, interim results (tentative hypotheses) may be  returned as they become available (these interim results are indicated with  the <code>is_final=false</code> flag).  If <code>false</code> or omitted, only <code>is_final=true</code> result(s) are returned.</p>","types":["boolean"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"RecognitionConfig","name":"RecognitionConfig","type":"instance","description":"<p>Provides information to the recognizer that specifies how to process the request.</p>","source":"packages\\speech\\src\\v1\\doc\\doc_cloud_speech.js#L177","resources":[],"examples":[],"params":[{"name":"encoding","description":"<p> <em>Required</em> Encoding of audio data sent in all <code>RecognitionAudio</code> messages.</p><p> The number should be among the values of <a ui-sref=\"docs.service({\n          serviceId: '{{ service.parent }}/{{service.path.split('/').shift()}}/data_types',\n          method: 'AudioEncoding'\n        })\">AudioEncoding</a></p>","types":["number"],"optional":false,"nullable":false},{"name":"sampleRateHertz","description":"<p> <em>Required</em> Sample rate in Hertz of the audio data sent in all  <code>RecognitionAudio</code> messages. Valid values are: 8000-48000.  16000 is optimal. For best results, set the sampling rate of the audio  source to 16000 Hz. If that&#39;s not possible, use the native sample rate of  the audio source (instead of re-sampling).</p>","types":["number"],"optional":false,"nullable":false},{"name":"languageCode","description":"<p> <em>Required</em> The language of the supplied audio as a  <a href=\"https://www.rfc-editor.org/rfc/bcp/bcp47.txt\">BCP-47</a> language tag.  Example: &quot;en-US&quot;.  See <a href=\"https://cloud.google.com/speech/docs/languages\">Language Support</a>  for a list of the currently supported language codes.</p>","types":["string"],"optional":false,"nullable":false},{"name":"maxAlternatives","description":"<p> <em>Optional</em> Maximum number of recognition hypotheses to be returned.  Specifically, the maximum number of <code>SpeechRecognitionAlternative</code> messages  within each <code>SpeechRecognitionResult</code>.  The server may return fewer than <code>max_alternatives</code>.  Valid values are <code>0</code>-<code>30</code>. A value of <code>0</code> or <code>1</code> will return a maximum of  one. If omitted, will return a maximum of one.</p>","types":["number"],"optional":false,"nullable":false},{"name":"profanityFilter","description":"<p> <em>Optional</em> If set to <code>true</code>, the server will attempt to filter out  profanities, replacing all but the initial character in each filtered word  with asterisks, e.g. &quot;f<em>*</em>&quot;. If set to <code>false</code> or omitted, profanities  won&#39;t be filtered out.</p>","types":["boolean"],"optional":false,"nullable":false},{"name":"speechContexts","description":"<p> <em>Optional</em> A means to provide context to assist the speech recognition.</p><p> This object should have the same structure as <a ui-sref=\"docs.service({\n          serviceId: '{{ service.parent }}/{{service.path.split('/').shift()}}/data_types',\n          method: 'SpeechContext'\n        })\">SpeechContext</a></p>","types":["Object[]"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"AudioEncoding","name":"AudioEncoding","type":"instance","description":"<p>Audio encoding of the data sent in the audio message. All encodings support only 1 channel (mono) audio. Only <code>FLAC</code> includes a header that describes the bytes of audio that follow the header. The other encodings are raw audio bytes with no header.</p><p>For best results, the audio source should be captured and transmitted using a lossless encoding (<code>FLAC</code> or <code>LINEAR16</code>). Recognition accuracy may be reduced if lossy codecs, which include the other codecs listed in this section, are used to capture or transmit the audio, particularly if background noise is present.</p>","source":"packages\\speech\\src\\v1\\doc\\doc_cloud_speech.js#L194","resources":[],"examples":[],"params":[],"exceptions":[],"returns":[]},{"id":"SpeechContext","name":"SpeechContext","type":"instance","description":"<p>Provides &quot;hints&quot; to the speech recognizer to favor specific words and phrases in the results.</p>","source":"packages\\speech\\src\\v1\\doc\\doc_cloud_speech.js#L272","resources":[],"examples":[],"params":[{"name":"phrases","description":"<p> <em>Optional</em> A list of strings containing words and phrases &quot;hints&quot; so that  the speech recognition is more likely to recognize them. This can be used  to improve the accuracy for specific words and phrases, for example, if  specific commands are typically spoken by the user. This can also be used  to add additional words to the vocabulary of the recognizer. See  <a href=\"https://cloud.google.com/speech/limits#content\">usage limits</a>.</p>","types":["string[]"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"RecognitionAudio","name":"RecognitionAudio","type":"instance","description":"<p>Contains audio data in the encoding specified in the <code>RecognitionConfig</code>. Either <code>content</code> or <code>uri</code> must be supplied. Supplying both or neither returns {@link google.rpc.Code.INVALID_ARGUMENT}. See <a href=\"https://cloud.google.com/speech/limits#content\">audio limits</a>.</p>","source":"packages\\speech\\src\\v1\\doc\\doc_cloud_speech.js#L298","resources":[],"examples":[],"params":[{"name":"content","description":"<p> The audio data bytes encoded as specified in  <code>RecognitionConfig</code>. Note: as with all bytes fields, protobuffers use a  pure binary representation, whereas JSON representations use base64.</p>","types":["string"],"optional":false,"nullable":false},{"name":"uri","description":"<p> URI that points to a file that contains audio data bytes as specified in  <code>RecognitionConfig</code>. Currently, only Google Cloud Storage URIs are  supported, which must be specified in the following format:  <code>gs://bucket_name/object_name</code> (other URI formats return  {@link google.rpc.Code.INVALID_ARGUMENT}). For more information, see  <a href=\"https://cloud.google.com/storage/docs/reference-uris\">Request URIs</a>.</p>","types":["string"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"RecognizeResponse","name":"RecognizeResponse","type":"instance","description":"","source":"packages\\speech\\src\\v1\\doc\\doc_cloud_speech.js#L316","resources":[],"examples":[],"params":[{"name":"results","description":"<p> <em>Output-only</em> Sequential list of transcription results corresponding to  sequential portions of audio.</p><p> This object should have the same structure as <a ui-sref=\"docs.service({\n          serviceId: '{{ service.parent }}/{{service.path.split('/').shift()}}/data_types',\n          method: 'SpeechRecognitionResult'\n        })\">SpeechRecognitionResult</a></p>","types":["Object[]"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"LongRunningRecognizeResponse","name":"LongRunningRecognizeResponse","type":"instance","description":"","source":"packages\\speech\\src\\v1\\doc\\doc_cloud_speech.js#L336","resources":[],"examples":[],"params":[{"name":"results","description":"<p> <em>Output-only</em> Sequential list of transcription results corresponding to  sequential portions of audio.</p><p> This object should have the same structure as <a ui-sref=\"docs.service({\n          serviceId: '{{ service.parent }}/{{service.path.split('/').shift()}}/data_types',\n          method: 'SpeechRecognitionResult'\n        })\">SpeechRecognitionResult</a></p>","types":["Object[]"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"LongRunningRecognizeMetadata","name":"LongRunningRecognizeMetadata","type":"instance","description":"<p>Describes the progress of a long-running <code>LongRunningRecognize</code> call. It is included in the <code>metadata</code> field of the <code>Operation</code> returned by the <code>GetOperation</code> call of the <code>google::longrunning::Operations</code> service.</p>","source":"packages\\speech\\src\\v1\\doc\\doc_cloud_speech.js#L362","resources":[],"examples":[],"params":[{"name":"progressPercent","description":"<p> Approximate percentage of audio processed thus far. Guaranteed to be 100  when the audio is fully processed and the results are available.</p>","types":["number"],"optional":false,"nullable":false},{"name":"startTime","description":"<p> Time when the request was received.</p><p> This object should have the same structure as google.protobuf.Timestamp</p>","types":["Object"],"optional":false,"nullable":false},{"name":"lastUpdateTime","description":"<p> Time of the most recent processing update.</p><p> This object should have the same structure as google.protobuf.Timestamp</p>","types":["Object"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"StreamingRecognizeResponse","name":"StreamingRecognizeResponse","type":"instance","description":"","source":"packages\\speech\\src\\v1\\doc\\doc_cloud_speech.js#L439","resources":[],"examples":[],"params":[{"name":"error","description":"<p> <em>Output-only</em> If set, returns a {@link google.rpc.Status} message that  specifies the error for the operation.</p><p> This object should have the same structure as google.rpc.Status</p>","types":["Object"],"optional":false,"nullable":false},{"name":"results","description":"<p> <em>Output-only</em> This repeated list contains zero or more results that  correspond to consecutive portions of the audio currently being processed.  It contains zero or one <code>is_final=true</code> result (the newly settled portion),  followed by zero or more <code>is_final=false</code> results.</p><p> This object should have the same structure as <a ui-sref=\"docs.service({\n          serviceId: '{{ service.parent }}/{{service.path.split('/').shift()}}/data_types',\n          method: 'StreamingRecognitionResult'\n        })\">StreamingRecognitionResult</a></p>","types":["Object[]"],"optional":false,"nullable":false},{"name":"speechEventType","description":"<p> <em>Output-only</em> Indicates the type of speech event.</p><p> The number should be among the values of <a ui-sref=\"docs.service({\n          serviceId: '{{ service.parent }}/{{service.path.split('/').shift()}}/data_types',\n          method: 'SpeechEventType'\n        })\">SpeechEventType</a></p>","types":["number"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"SpeechEventType","name":"SpeechEventType","type":"instance","description":"<p>Indicates the type of speech event.</p>","source":"packages\\speech\\src\\v1\\doc\\doc_cloud_speech.js#L447","resources":[],"examples":[],"params":[],"exceptions":[],"returns":[]},{"id":"StreamingRecognitionResult","name":"StreamingRecognitionResult","type":"instance","description":"<p>A streaming speech recognition result corresponding to a portion of the audio that is currently being processed.</p>","source":"packages\\speech\\src\\v1\\doc\\doc_cloud_speech.js#L494","resources":[],"examples":[],"params":[{"name":"alternatives","description":"<p> <em>Output-only</em> May contain one or more recognition hypotheses (up to the  maximum specified in <code>max_alternatives</code>).</p><p> This object should have the same structure as <a ui-sref=\"docs.service({\n          serviceId: '{{ service.parent }}/{{service.path.split('/').shift()}}/data_types',\n          method: 'SpeechRecognitionAlternative'\n        })\">SpeechRecognitionAlternative</a></p>","types":["Object[]"],"optional":false,"nullable":false},{"name":"isFinal","description":"<p> <em>Output-only</em> If <code>false</code>, this <code>StreamingRecognitionResult</code> represents an  interim result that may change. If <code>true</code>, this is the final time the  speech service will return this particular <code>StreamingRecognitionResult</code>,  the recognizer will not return any further hypotheses for this portion of  the transcript and corresponding audio.</p>","types":["boolean"],"optional":false,"nullable":false},{"name":"stability","description":"<p> <em>Output-only</em> An estimate of the likelihood that the recognizer will not  change its guess about this interim result. Values range from 0.0  (completely unstable) to 1.0 (completely stable).  This field is only provided for interim results (<code>is_final=false</code>).  The default of 0.0 is a sentinel value indicating <code>stability</code> was not set.</p>","types":["number"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"SpeechRecognitionResult","name":"SpeechRecognitionResult","type":"instance","description":"<p>A speech recognition result corresponding to a portion of the audio.</p>","source":"packages\\speech\\src\\v1\\doc\\doc_cloud_speech.js#L510","resources":[],"examples":[],"params":[{"name":"alternatives","description":"<p> <em>Output-only</em> May contain one or more recognition hypotheses (up to the  maximum specified in <code>max_alternatives</code>).</p><p> This object should have the same structure as <a ui-sref=\"docs.service({\n          serviceId: '{{ service.parent }}/{{service.path.split('/').shift()}}/data_types',\n          method: 'SpeechRecognitionAlternative'\n        })\">SpeechRecognitionAlternative</a></p>","types":["Object[]"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"SpeechRecognitionAlternative","name":"SpeechRecognitionAlternative","type":"instance","description":"<p>Alternative hypotheses (a.k.a. n-best list).</p>","source":"packages\\speech\\src\\v1\\doc\\doc_cloud_speech.js#L532","resources":[],"examples":[],"params":[{"name":"transcript","description":"<p> <em>Output-only</em> Transcript text representing the words that the user spoke.</p>","types":["string"],"optional":false,"nullable":false},{"name":"confidence","description":"<p> <em>Output-only</em> The confidence estimate between 0.0 and 1.0. A higher number  indicates an estimated greater likelihood that the recognized words are  correct. This field is typically provided only for the top hypothesis, and  only for <code>is_final=true</code> results. Clients should not rely on the  <code>confidence</code> field as it is not guaranteed to be accurate, or even set, in  any of the results.  The default of 0.0 is a sentinel value indicating <code>confidence</code> was not set.</p>","types":["number"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]}],"path":"doc_cloud_speech.json"}