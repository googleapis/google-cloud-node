{"name":"Data Types","methods":[{"id":"RecognizeRequest","name":"RecognizeRequest","type":"instance","description":"","source":"packages\\speech\\src\\v1\\doc\\doc_cloud_speech.js#L39","resources":[],"examples":[],"params":[{"name":"config","description":"<p> <em>Required</em> Provides information to the recognizer that specifies how to  process the request.</p><p> This object should have the same structure as <a ui-sref=\"docs.service({\n          serviceId: '{{ service.parent }}/{{service.path.split('/').shift()}}/data_types',\n          method: 'RecognitionConfig'\n        })\">RecognitionConfig</a></p>","types":["Object"],"optional":false,"nullable":false},{"name":"audio","description":"<p> <em>Required</em> The audio data to be recognized.</p><p> This object should have the same structure as <a ui-sref=\"docs.service({\n          serviceId: '{{ service.parent }}/{{service.path.split('/').shift()}}/data_types',\n          method: 'RecognitionAudio'\n        })\">RecognitionAudio</a></p>","types":["Object"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"LongRunningRecognizeRequest","name":"LongRunningRecognizeRequest","type":"instance","description":"","source":"packages\\speech\\src\\v1\\doc\\doc_cloud_speech.js#L61","resources":[],"examples":[],"params":[{"name":"config","description":"<p> <em>Required</em> Provides information to the recognizer that specifies how to  process the request.</p><p> This object should have the same structure as <a ui-sref=\"docs.service({\n          serviceId: '{{ service.parent }}/{{service.path.split('/').shift()}}/data_types',\n          method: 'RecognitionConfig'\n        })\">RecognitionConfig</a></p>","types":["Object"],"optional":false,"nullable":false},{"name":"audio","description":"<p> <em>Required</em> The audio data to be recognized.</p><p> This object should have the same structure as <a ui-sref=\"docs.service({\n          serviceId: '{{ service.parent }}/{{service.path.split('/').shift()}}/data_types',\n          method: 'RecognitionAudio'\n        })\">RecognitionAudio</a></p>","types":["Object"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"StreamingRecognizeRequest","name":"StreamingRecognizeRequest","type":"instance","description":"","source":"packages\\speech\\src\\v1\\doc\\doc_cloud_speech.js#L92","resources":[],"examples":[],"params":[{"name":"streamingConfig","description":"<p> Provides information to the recognizer that specifies how to process the  request. The first <code>StreamingRecognizeRequest</code> message must contain a  <code>streaming_config</code> message.</p><p> This object should have the same structure as <a ui-sref=\"docs.service({\n          serviceId: '{{ service.parent }}/{{service.path.split('/').shift()}}/data_types',\n          method: 'StreamingRecognitionConfig'\n        })\">StreamingRecognitionConfig</a></p>","types":["Object"],"optional":false,"nullable":false},{"name":"audioContent","description":"<p> The audio data to be recognized. Sequential chunks of audio data are sent  in sequential <code>StreamingRecognizeRequest</code> messages. The first  <code>StreamingRecognizeRequest</code> message must not contain <code>audio_content</code> data  and all subsequent <code>StreamingRecognizeRequest</code> messages must contain  <code>audio_content</code> data. The audio bytes must be encoded as specified in  <code>RecognitionConfig</code>. Note: as with all bytes fields, protobuffers use a  pure binary representation (not base64). See  <a href=\"https://cloud.google.com/speech/limits#content\">audio limits</a>.</p>","types":["string"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"StreamingRecognitionConfig","name":"StreamingRecognitionConfig","type":"instance","description":"<p>Provides information to the recognizer that specifies how to process the request.</p>","source":"packages\\speech\\src\\v1\\doc\\doc_cloud_speech.js#L128","resources":[],"examples":[],"params":[{"name":"config","description":"<p> <em>Required</em> Provides information to the recognizer that specifies how to  process the request.</p><p> This object should have the same structure as <a ui-sref=\"docs.service({\n          serviceId: '{{ service.parent }}/{{service.path.split('/').shift()}}/data_types',\n          method: 'RecognitionConfig'\n        })\">RecognitionConfig</a></p>","types":["Object"],"optional":false,"nullable":false},{"name":"singleUtterance","description":"<p> <em>Optional</em> If <code>false</code> or omitted, the recognizer will perform continuous  recognition (continuing to wait for and process audio even if the user  pauses speaking) until the client closes the input stream (gRPC API) or  until the maximum time limit has been reached. May return multiple  <code>StreamingRecognitionResult</code>s with the <code>is_final</code> flag set to <code>true</code>.</p><p> If <code>true</code>, the recognizer will detect a single spoken utterance. When it  detects that the user has paused or stopped speaking, it will return an  <code>END_OF_SINGLE_UTTERANCE</code> event and cease recognition. It will return no  more than one <code>StreamingRecognitionResult</code> with the <code>is_final</code> flag set to  <code>true</code>.</p>","types":["boolean"],"optional":false,"nullable":false},{"name":"interimResults","description":"<p> <em>Optional</em> If <code>true</code>, interim results (tentative hypotheses) may be  returned as they become available (these interim results are indicated with  the <code>is_final=false</code> flag).  If <code>false</code> or omitted, only <code>is_final=true</code> result(s) are returned.</p>","types":["boolean"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"RecognitionConfig","name":"RecognitionConfig","type":"instance","description":"<p>Provides information to the recognizer that specifies how to process the request.</p>","source":"packages\\speech\\src\\v1\\doc\\doc_cloud_speech.js#L177","resources":[],"examples":[],"params":[{"name":"encoding","description":"<p> <em>Required</em> Encoding of audio data sent in all <code>RecognitionAudio</code> messages.</p><p> The number should be among the values of <a ui-sref=\"docs.service({\n          serviceId: '{{ service.parent }}/{{service.path.split('/').shift()}}/data_types',\n          method: 'AudioEncoding'\n        })\">AudioEncoding</a></p>","types":["number"],"optional":false,"nullable":false},{"name":"sampleRateHertz","description":"<p> <em>Required</em> Sample rate in Hertz of the audio data sent in all  <code>RecognitionAudio</code> messages. Valid values are: 8000-48000.  16000 is optimal. For best results, set the sampling rate of the audio  source to 16000 Hz. If that&#39;s not possible, use the native sample rate of  the audio source (instead of re-sampling).</p>","types":["number"],"optional":false,"nullable":false},{"name":"languageCode","description":"<p> <em>Required</em> The language of the supplied audio as a  <a href=\"https://www.rfc-editor.org/rfc/bcp/bcp47.txt\">BCP-47</a> language tag.  Example: &quot;en-US&quot;.  See <a href=\"https://cloud.google.com/speech/docs/languages\">Language Support</a>  for a list of the currently supported language codes.</p>","types":["string"],"optional":false,"nullable":false},{"name":"maxAlternatives","description":"<p> <em>Optional</em> Maximum number of recognition hypotheses to be returned.  Specifically, the maximum number of <code>SpeechRecognitionAlternative</code> messages  within each <code>SpeechRecognitionResult</code>.  The server may return fewer than <code>max_alternatives</code>.  Valid values are <code>0</code>-<code>30</code>. A value of <code>0</code> or <code>1</code> will return a maximum of  one. If omitted, will return a maximum of one.</p>","types":["number"],"optional":false,"nullable":false},{"name":"profanityFilter","description":"<p> <em>Optional</em> If set to <code>true</code>, the server will attempt to filter out  profanities, replacing all but the initial character in each filtered word  with asterisks, e.g. &quot;f<em>*</em>&quot;. If set to <code>false</code> or omitted, profanities  won&#39;t be filtered out.</p>","types":["boolean"],"optional":false,"nullable":false},{"name":"speechContexts","description":"<p> <em>Optional</em> A means to provide context to assist the speech recognition.</p><p> This object should have the same structure as <a ui-sref=\"docs.service({\n          serviceId: '{{ service.parent }}/{{service.path.split('/').shift()}}/data_types',\n          method: 'SpeechContext'\n        })\">SpeechContext</a></p>","types":["Object[]"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"AudioEncoding","name":"AudioEncoding","type":"instance","description":"<p>Audio encoding of the data sent in the audio message. All encodings support only 1 channel (mono) audio. Only <code>FLAC</code> includes a header that describes the bytes of audio that follow the header. The other encodings are raw audio bytes with no header.</p><p>For best results, the audio source should be captured and transmitted using a lossless encoding (<code>FLAC</code> or <code>LINEAR16</code>). Recognition accuracy may be reduced if lossy codecs, which include the other codecs listed in this section, are used to capture or transmit the audio, particularly if background noise is present.</p>","source":"packages\\speech\\src\\v1\\doc\\doc_cloud_speech.js#L194","resources":[],"examples":[],"params":[],"exceptions":[],"returns":[]},{"id":"SpeechContext","name":"SpeechContext","type":"instance","description":"<p>Provides &quot;hints&quot; to the speech recognizer to favor specific words and phrases in the results.</p>","source":"packages\\speech\\src\\v1\\doc\\doc_cloud_speech.js#L272","resources":[],"examples":[],"params":[{"name":"phrases","description":"<p> <em>Optional</em> A list of strings containing words and phrases &quot;hints&quot; so that  the speech recognition is more likely to recognize them. This can be used  to improve the accuracy for specific words and phrases, for example, if  specific commands are typically spoken by the user. This can also be used  to add additional words to the vocabulary of the recognizer. See  <a href=\"https://cloud.google.com/speech/limits#content\">usage limits</a>.</p>","types":["string[]"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"RecognitionAudio","name":"RecognitionAudio","type":"instance","description":"<p>Contains audio data in the encoding specified in the <code>RecognitionConfig</code>. Either <code>content</code> or <code>uri</code> must be supplied. Supplying both or neither returns {@link google.rpc.Code.INVALID_ARGUMENT}. See <a href=\"https://cloud.google.com/speech/limits#content\">audio limits</a>.</p>","source":"packages\\speech\\src\\v1\\doc\\doc_cloud_speech.js#L298","resources":[],"examples":[],"params":[{"name":"content","description":"<p> The audio data bytes encoded as specified in  <code>RecognitionConfig</code>. Note: as with all bytes fields, protobuffers use a  pure binary representation, whereas JSON representations use base64.</p>","types":["string"],"optional":false,"nullable":false},{"name":"uri","description":"<p> URI that points to a file that contains audio data bytes as specified in  <code>RecognitionConfig</code>. Currently, only Google Cloud Storage URIs are  supported, which must be specified in the following format:  <code>gs://bucket_name/object_name</code> (other URI formats return  {@link google.rpc.Code.INVALID_ARGUMENT}). For more information, see  <a href=\"https://cloud.google.com/storage/docs/reference-uris\">Request URIs</a>.</p>","types":["string"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"RecognizeResponse","name":"RecognizeResponse","type":"instance","description":"","source":"packages\\speech\\src\\v1\\doc\\doc_cloud_speech.js#L316","resources":[],"examples":[],"params":[{"name":"results","description":"<p> <em>Output-only</em> Sequential list of transcription results corresponding to  sequential portions of audio.</p><p> This object should have the same structure as <a ui-sref=\"docs.service({\n          serviceId: '{{ service.parent }}/{{service.path.split('/').shift()}}/data_types',\n          method: 'SpeechRecognitionResult'\n        })\">SpeechRecognitionResult</a></p>","types":["Object[]"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"LongRunningRecognizeResponse","name":"LongRunningRecognizeResponse","type":"instance","description":"","source":"packages\\speech\\src\\v1\\doc\\doc_cloud_speech.js#L336","resources":[],"examples":[],"params":[{"name":"results","description":"<p> <em>Output-only</em> Sequential list of transcription results corresponding to  sequential portions of audio.</p><p> This object should have the same structure as <a ui-sref=\"docs.service({\n          serviceId: '{{ service.parent }}/{{service.path.split('/').shift()}}/data_types',\n          method: 'SpeechRecognitionResult'\n        })\">SpeechRecognitionResult</a></p>","types":["Object[]"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"LongRunningRecognizeMetadata","name":"LongRunningRecognizeMetadata","type":"instance","description":"<p>Describes the progress of a long-running <code>LongRunningRecognize</code> call. It is included in the <code>metadata</code> field of the <code>Operation</code> returned by the <code>GetOperation</code> call of the <code>google::longrunning::Operations</code> service.</p>","source":"packages\\speech\\src\\v1\\doc\\doc_cloud_speech.js#L362","resources":[],"examples":[],"params":[{"name":"progressPercent","description":"<p> Approximate percentage of audio processed thus far. Guaranteed to be 100  when the audio is fully processed and the results are available.</p>","types":["number"],"optional":false,"nullable":false},{"name":"startTime","description":"<p> Time when the request was received.</p><p> This object should have the same structure as google.protobuf.Timestamp</p>","types":["Object"],"optional":false,"nullable":false},{"name":"lastUpdateTime","description":"<p> Time of the most recent processing update.</p><p> This object should have the same structure as google.protobuf.Timestamp</p>","types":["Object"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"StreamingRecognizeResponse","name":"StreamingRecognizeResponse","type":"instance","description":"","source":"packages\\speech\\src\\v1\\doc\\doc_cloud_speech.js#L439","resources":[],"examples":[],"params":[{"name":"error","description":"<p> <em>Output-only</em> If set, returns a {@link google.rpc.Status} message that  specifies the error for the operation.</p><p> This object should have the same structure as google.rpc.Status</p>","types":["Object"],"optional":false,"nullable":false},{"name":"results","description":"<p> <em>Output-only</em> This repeated list contains zero or more results that  correspond to consecutive portions of the audio currently being processed.  It contains zero or one <code>is_final=true</code> result (the newly settled portion),  followed by zero or more <code>is_final=false</code> results.</p><p> This object should have the same structure as <a ui-sref=\"docs.service({\n          serviceId: '{{ service.parent }}/{{service.path.split('/').shift()}}/data_types',\n          method: 'StreamingRecognitionResult'\n        })\">StreamingRecognitionResult</a></p>","types":["Object[]"],"optional":false,"nullable":false},{"name":"speechEventType","description":"<p> <em>Output-only</em> Indicates the type of speech event.</p><p> The number should be among the values of <a ui-sref=\"docs.service({\n          serviceId: '{{ service.parent }}/{{service.path.split('/').shift()}}/data_types',\n          method: 'SpeechEventType'\n        })\">SpeechEventType</a></p>","types":["number"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"SpeechEventType","name":"SpeechEventType","type":"instance","description":"<p>Indicates the type of speech event.</p>","source":"packages\\speech\\src\\v1\\doc\\doc_cloud_speech.js#L447","resources":[],"examples":[],"params":[],"exceptions":[],"returns":[]},{"id":"StreamingRecognitionResult","name":"StreamingRecognitionResult","type":"instance","description":"<p>A streaming speech recognition result corresponding to a portion of the audio that is currently being processed.</p>","source":"packages\\speech\\src\\v1\\doc\\doc_cloud_speech.js#L494","resources":[],"examples":[],"params":[{"name":"alternatives","description":"<p> <em>Output-only</em> May contain one or more recognition hypotheses (up to the  maximum specified in <code>max_alternatives</code>).</p><p> This object should have the same structure as <a ui-sref=\"docs.service({\n          serviceId: '{{ service.parent }}/{{service.path.split('/').shift()}}/data_types',\n          method: 'SpeechRecognitionAlternative'\n        })\">SpeechRecognitionAlternative</a></p>","types":["Object[]"],"optional":false,"nullable":false},{"name":"isFinal","description":"<p> <em>Output-only</em> If <code>false</code>, this <code>StreamingRecognitionResult</code> represents an  interim result that may change. If <code>true</code>, this is the final time the  speech service will return this particular <code>StreamingRecognitionResult</code>,  the recognizer will not return any further hypotheses for this portion of  the transcript and corresponding audio.</p>","types":["boolean"],"optional":false,"nullable":false},{"name":"stability","description":"<p> <em>Output-only</em> An estimate of the likelihood that the recognizer will not  change its guess about this interim result. Values range from 0.0  (completely unstable) to 1.0 (completely stable).  This field is only provided for interim results (<code>is_final=false</code>).  The default of 0.0 is a sentinel value indicating <code>stability</code> was not set.</p>","types":["number"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"SpeechRecognitionResult","name":"SpeechRecognitionResult","type":"instance","description":"<p>A speech recognition result corresponding to a portion of the audio.</p>","source":"packages\\speech\\src\\v1\\doc\\doc_cloud_speech.js#L510","resources":[],"examples":[],"params":[{"name":"alternatives","description":"<p> <em>Output-only</em> May contain one or more recognition hypotheses (up to the  maximum specified in <code>max_alternatives</code>).</p><p> This object should have the same structure as <a ui-sref=\"docs.service({\n          serviceId: '{{ service.parent }}/{{service.path.split('/').shift()}}/data_types',\n          method: 'SpeechRecognitionAlternative'\n        })\">SpeechRecognitionAlternative</a></p>","types":["Object[]"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"id":"SpeechRecognitionAlternative","name":"SpeechRecognitionAlternative","type":"instance","description":"<p>Alternative hypotheses (a.k.a. n-best list).</p>","source":"packages\\speech\\src\\v1\\doc\\doc_cloud_speech.js#L532","resources":[],"examples":[],"params":[{"name":"transcript","description":"<p> <em>Output-only</em> Transcript text representing the words that the user spoke.</p>","types":["string"],"optional":false,"nullable":false},{"name":"confidence","description":"<p> <em>Output-only</em> The confidence estimate between 0.0 and 1.0. A higher number  indicates an estimated greater likelihood that the recognized words are  correct. This field is typically provided only for the top hypothesis, and  only for <code>is_final=true</code> results. Clients should not rely on the  <code>confidence</code> field as it is not guaranteed to be accurate, or even set, in  any of the results.  The default of 0.0 is a sentinel value indicating <code>confidence</code> was not set.</p>","types":["number"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"type":"instance","description":"<p><code>Any</code> contains an arbitrary serialized protocol buffer message along with a URL that describes the type of the serialized message.</p><p>Protobuf library provides support to pack/unpack Any values in the form of utility functions or additional generated methods of the Any type.</p><p>Example 1: Pack and unpack a message in C++.</p><pre><code>Foo foo = ...; Any any; any.PackFrom(foo); ... if (any.UnpackTo(&amp;foo)) { ... } </code></pre><p>Example 2: Pack and unpack a message in Java.</p><pre><code>Foo foo = ...; Any any = Any.pack(foo); ... if (any.is(Foo.class)) { foo = any.unpack(Foo.class); } </code></pre><p> Example 3: Pack and unpack a message in Python.</p><pre><code>foo = Foo(...) any = Any() any.Pack(foo) ... if any.Is(Foo.DESCRIPTOR): any.Unpack(foo) ... </code></pre><p>The pack methods provided by protobuf library will by default use &#39;type.googleapis.com/full.type.name&#39; as the type URL and the unpack methods only use the fully qualified type name after the last &#39;/&#39; in the type URL, for example &quot;foo.bar.com/x/y.z&quot; will yield type name &quot;y.z&quot;.</p><h1>JSON</h1> <p>The JSON representation of an <code>Any</code> value uses the regular representation of the deserialized, embedded message, with an additional field <code>@type</code> which contains the type URL. Example:</p><pre><code>package google.profile; message Person { string first_name = 1; string last_name = 2; } { &quot;@type&quot;: &quot;type.googleapis.com/google.profile.Person&quot;, &quot;firstName&quot;: &lt;string&gt;, &quot;lastName&quot;: &lt;string&gt; } </code></pre><p>If the embedded message type is well-known and has a custom JSON representation, that representation will be embedded adding a field <code>value</code> which holds the custom JSON in addition to the <code>@type</code> field. Example (for message {@link google.protobuf.Duration}):</p><pre><code>{ &quot;@type&quot;: &quot;type.googleapis.com/google.protobuf.Duration&quot;, &quot;value&quot;: &quot;1.212s&quot; } </code></pre>","source":"packages\\speech\\src\\v1\\doc\\doc_google_protobuf_any.js#L122","resources":[],"examples":[],"params":[{"name":"typeUrl","description":"<p> A URL/resource name whose content describes the type of the  serialized protocol buffer message.</p><p> For URLs which use the scheme <code>http</code>, <code>https</code>, or no scheme, the  following restrictions and interpretations apply:</p><ul> <li>If no scheme is provided, <code>https</code> is assumed.</li> <li>The last segment of the URL&#39;s path must represent the fully qualified name of the type (as in <code>path/google.protobuf.Duration</code>). The name should be in a canonical form (e.g., leading &quot;.&quot; is not accepted).</li> <li>An HTTP GET on the URL must yield a {@link google.protobuf.Type} value in binary format, or produce an error.</li> <li><p>Applications are allowed to cache lookup results based on the URL, or have them precompiled into a binary to avoid any lookup. Therefore, binary compatibility needs to be preserved on changes to types. (Use versioned type names to manage breaking changes.)</p><p>Schemes other than <code>http</code>, <code>https</code> (or the empty scheme) might be used with implementation specific semantics.</p></li> </ul> ","types":["string"],"optional":false,"nullable":false},{"name":"value","description":"<p> Must be a valid serialized protocol buffer of the above specified type.</p>","types":["string"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]},{"type":"instance","description":"<p>The <code>Status</code> type defines a logical error model that is suitable for different programming environments, including REST APIs and RPC APIs. It is used by <a href=\"https://github.com/grpc\">gRPC</a>. The error model is designed to be:</p><ul> <li>Simple to use and understand for most users</li> <li>Flexible enough to meet unexpected needs</li> </ul> <h1>Overview</h1> <p>The <code>Status</code> message contains three pieces of data: error code, error message, and error details. The error code should be an enum value of {@link google.rpc.Code}, but it may accept additional error codes if needed. The error message should be a developer-facing English message that helps developers <em>understand</em> and <em>resolve</em> the error. If a localized user-facing error message is needed, put the localized message in the error details or localize it in the client. The optional error details may contain arbitrary information about the error. There is a predefined set of error detail types in the package <code>google.rpc</code> which can be used for common error conditions.</p><h1>Language mapping</h1> <p>The <code>Status</code> message is the logical representation of the error model, but it is not necessarily the actual wire format. When the <code>Status</code> message is exposed in different client libraries and different wire protocols, it can be mapped differently. For example, it will likely be mapped to some exceptions in Java, but more likely mapped to some error codes in C.</p><h1>Other uses</h1> <p>The error model and the <code>Status</code> message can be used in a variety of environments, either with or without APIs, to provide a consistent developer experience across different environments.</p><p>Example uses of this error model include:</p><ul> <li><p>Partial errors. If a service needs to return partial errors to the client,  it may embed the <code>Status</code> in the normal response to indicate the partial  errors.</p></li> <li><p>Workflow errors. A typical workflow has multiple steps. Each step may  have a <code>Status</code> message for error reporting purpose.</p></li> <li><p>Batch operations. If a client uses batch request and batch response, the  <code>Status</code> message should be used directly inside batch response, one for  each error sub-response.</p></li> <li><p>Asynchronous operations. If an API call embeds asynchronous operation  results in its response, the status of those operations should be  represented directly using the <code>Status</code> message.</p></li> <li><p>Logging. If some API errors are stored in logs, the message <code>Status</code> could  be used directly after any stripping needed for security/privacy reasons.</p></li> </ul> ","source":"packages\\speech\\src\\v1\\doc\\doc_google_rpc_status.js#L93","resources":[],"examples":[],"params":[{"name":"code","description":"<p> The status code, which should be an enum value of {@link google.rpc.Code}.</p>","types":["number"],"optional":false,"nullable":false},{"name":"message","description":"<p> A developer-facing error message, which should be in English. Any  user-facing error message should be localized and sent in the  {@link google.rpc.Status.details} field, or localized by the client.</p>","types":["string"],"optional":false,"nullable":false},{"name":"details","description":"<p> A list of messages that carry the error details. There will be a  common set of message types for APIs to use.</p><p> This object should have the same structure as google.protobuf.Any</p>","types":["Object[]"],"optional":false,"nullable":false}],"exceptions":[],"returns":[]}],"path":"v1/data_types.json","description":"\n          <table class=\"table\">\n            <thead>\n              <tr>\n                <th>Class</th>\n                <th>Description</th>\n              </tr>\n            </thead>\n            <tbody>\n              <tr ng-repeat=\"method in service.methods\" ng-if=\"method.name\">\n                <td>\n                  <a ui-sref=\"docs.service({ method: method.id })\" class=\"skip-external-link\">\n                    {{method.name}}\n                  </a>\n                </td>\n                <td>\n                  <span ng-bind-html=\"method.description\">\n                    {{method.description}}\n                  </span>\n                  <span ng-if=\"!method.description && method.name.includes('Request')\">\n                    The request for {{method.name}}.\n                  </span>\n                  <span ng-if=\"!method.description && method.name.includes('Response')\">\n                    The response for {{method.name}}.\n                  </span>\n                </td>\n              </tr>\n            </tbody>\n          </table>\n        ","id":"speech/v1/data_types"}